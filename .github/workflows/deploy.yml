name: Deploy to EC2

on:
  push:
    branches:
      - main
  workflow_dispatch:

jobs:
  deploy:
    runs-on: ubuntu-latest
    timeout-minutes: 60

    steps:
      - name: Deploy to EC2
        uses: appleboy/ssh-action@v1.0.3
        with:
          host: ${{ secrets.EC2_HOST }}
          username: ubuntu
          key: ${{ secrets.EC2_SSH_KEY }}
          port: 22
          command_timeout: 55m
          script: |
            echo "ğŸš€ Starting deployment..."

            # Ensure we're in the right directory
            if [ -d ~/opdwallet_aws ]; then
              cd ~/opdwallet_aws
              echo "ğŸ“¥ Fetching latest code from remote..."
              git fetch origin

              echo "ğŸ”„ Force-resetting to remote main (discarding any local changes)..."
              sudo git reset --hard origin/main
              echo "ğŸ§¹ Cleaning untracked files..."
              sudo git clean -fd
            else
              echo "ğŸ“ Cloning repository..."
              cd ~
              git clone https://github.com/anilkumar1510/opdwallet_aws.git
              cd opdwallet_aws
            fi

            # Ensure deployment script is executable
            chmod +x deploy-production.sh deploy-on-aws.sh scripts/cleanup-containers.sh 2>/dev/null || true

            # Run comprehensive cleanup to prevent port conflicts
            echo "ğŸ§¹ Running container cleanup to prevent port conflicts..."
            ./scripts/cleanup-containers.sh || echo "âš ï¸ Cleanup script not found, proceeding with manual cleanup"

            # Manual fallback cleanup if script fails
            echo "ğŸ›‘ Stopping any existing OPD containers..."
            docker stop $(docker ps -q --filter name="opd-") 2>/dev/null || true
            docker rm $(docker ps -aq --filter name="opd-") 2>/dev/null || true

            # Detect port conflicts before deployment
            echo "ğŸ” Checking for port conflicts on 4000, 3001, 3002, 3004, 3005, 3006, 6379..."
            CONFLICTS=$(netstat -tuln 2>/dev/null | grep -E ":(4000|3001|3002|3004|3005|3006|6379|80|443) " || true)
            if [ ! -z "$CONFLICTS" ]; then
              echo "âš ï¸ Warning: Found services using deployment ports:"
              echo "$CONFLICTS"
              echo "ğŸ”„ Attempting to resolve conflicts..."

              # Kill processes using our ports
              sudo fuser -k 4000/tcp 2>/dev/null || true
              sudo fuser -k 3001/tcp 2>/dev/null || true
              sudo fuser -k 3002/tcp 2>/dev/null || true
              sudo fuser -k 3004/tcp 2>/dev/null || true
              sudo fuser -k 3005/tcp 2>/dev/null || true
              sudo fuser -k 3006/tcp 2>/dev/null || true
              sudo fuser -k 6379/tcp 2>/dev/null || true
              sudo fuser -k 80/tcp 2>/dev/null || true
              sudo fuser -k 443/tcp 2>/dev/null || true

              sleep 5
            else
              echo "âœ… No port conflicts detected"
            fi

            # Clean up Docker to free memory (preserves build cache and volumes including mongo-data-prod and redis-data-prod)
            # Only remove stopped containers, dangling images, and unused networks (NOT build cache)
            echo "ğŸ§¹ Cleaning up stopped containers and dangling images (preserving build cache and data volumes)..."
            docker container prune -f 2>/dev/null || true
            docker image prune -f 2>/dev/null || true
            docker network prune -f 2>/dev/null || true

            # Ensure we're in the project directory
            cd ~/opdwallet_aws || { echo "âŒ ERROR: Cannot access opdwallet_aws directory"; exit 1; }

            # Setup environment variables - Docker Compose needs .env file
            echo "ğŸ“ Setting up environment variables..."
            if [ -f .env.production ]; then
              cp .env.production .env
              echo "âœ… Environment variables configured from .env.production"
            else
              echo "âŒ ERROR: .env.production file not found"
              exit 1
            fi

            # Setup uploads directories with proper permissions
            echo "ğŸ“ Setting up upload directories with proper permissions..."
            mkdir -p api/uploads/claims
            mkdir -p api/uploads/lab-prescriptions
            mkdir -p api/uploads/lab-reports
            mkdir -p api/uploads/doctors
            mkdir -p api/uploads/prescriptions
            mkdir -p api/uploads/prescriptions/generated
            mkdir -p api/uploads/diagnostic-prescriptions
            mkdir -p api/uploads/diagnostic-reports

            # Set permissions so Docker container can write
            # Using sudo because existing files may be owned by root (from Docker containers)
            sudo chmod -R 777 api/uploads 2>/dev/null || true
            echo "âœ… Upload directories created with write permissions"

            # Pre-pull critical images to avoid Docker Hub 503 errors during compose
            echo "ğŸ“¦ Pre-pulling critical Docker images..."
            docker pull nginx:alpine 2>/dev/null || echo "âš ï¸ Warning: Failed to pre-pull nginx:alpine, will retry during compose"
            docker pull mongo:7.0 2>/dev/null || echo "âš ï¸ Warning: Failed to pre-pull mongo:7.0, will retry during compose"
            docker pull redis:7-alpine 2>/dev/null || echo "âš ï¸ Warning: Failed to pre-pull redis:7-alpine, will retry during compose"
            docker pull node:20-alpine 2>/dev/null || echo "âš ï¸ Warning: Failed to pre-pull node:20-alpine, will retry during compose"

            # Build and run with retry mechanism for Docker Hub intermittent issues
            # Using COMPOSE_PARALLEL_LIMIT to limit concurrent builds and prevent memory exhaustion
            echo "ğŸ”¨ Building and starting services (max 2 parallel builds to prevent OOM)..."
            export COMPOSE_PARALLEL_LIMIT=2
            MAX_RETRIES=3
            RETRY_COUNT=0

            while [ $RETRY_COUNT -lt $MAX_RETRIES ]; do
              if docker compose -f docker-compose.prod.yml up -d --build --remove-orphans; then
                echo "âœ… Docker-compose succeeded"
                break
              else
                RETRY_COUNT=$((RETRY_COUNT + 1))
                if [ $RETRY_COUNT -lt $MAX_RETRIES ]; then
                  WAIT_TIME=$((RETRY_COUNT * 10))
                  echo "âš ï¸ Docker-compose failed (attempt $RETRY_COUNT/$MAX_RETRIES), retrying in ${WAIT_TIME}s..."
                  sleep $WAIT_TIME
                else
                  echo "âŒ ERROR: Docker-compose failed after $MAX_RETRIES attempts"
                  exit 1
                fi
              fi
            done

            # Wait for services
            sleep 30

            # Verify no port conflicts after deployment
            echo "ğŸ” Post-deployment port conflict check..."
            CONFLICTS_POST=$(docker ps --format "table {{.Names}}\t{{.Ports}}" | grep -E ":4000.*:4000" | wc -l || echo "0")
            if [ "$CONFLICTS_POST" -gt 1 ]; then
              echo "âŒ ERROR: Multiple containers detected on port 4000!"
              docker ps --format "table {{.Names}}\t{{.Ports}}" | grep ":4000"
              exit 1
            else
              echo "âœ… No port 4000 conflicts detected"
            fi

            # Check status
            echo "ğŸ“Š Deployment status:"
            docker ps

            # Create admin user if not exists
            echo "ğŸ‘¤ Creating admin user if not exists..."
            docker exec opd-api-prod node -e "
            const bcrypt = require('bcrypt');
            const { MongoClient, ObjectId } = require('mongodb');

            async function createAdmin() {
              const uri = 'mongodb://mongodb:27017/opd_wallet';
              const client = new MongoClient(uri);
              try {
                await client.connect();
                const db = client.db('opd_wallet');
                const users = db.collection('users');

                const existing = await users.findOne({ email: 'admin@opdwallet.com' });
                if (existing) {
                  console.log('âœ… Admin user already exists');
                  return;
                }

                const passwordHash = await bcrypt.hash('Admin@123', 12);
                const timestamp = Date.now();

                await users.insertOne({
                  _id: new ObjectId(),
                  userId: \`ADMIN-\${timestamp}\`,
                  uhid: \`UHID-ADMIN-\${timestamp}\`,
                  memberId: \`MEM-ADMIN-\${timestamp}\`,
                  relationship: 'SELF',
                  name: { firstName: 'Super', lastName: 'Administrator', fullName: 'Super Administrator' },
                  email: 'admin@opdwallet.com',
                  phone: '+919999999999',
                  dob: new Date('1990-01-01'),
                  gender: 'MALE',
                  role: 'SUPER_ADMIN',
                  status: 'ACTIVE',
                  passwordHash: passwordHash,
                  mustChangePassword: false,
                  createdAt: new Date(),
                  updatedAt: new Date()
                });

                console.log('âœ… Admin user created: admin@opdwallet.com / Admin@123');
              } catch (error) {
                console.error('âŒ Admin user creation error:', error.message);
              } finally {
                await client.close();
              }
            }

            createAdmin().catch(console.error);
            " 2>&1 || echo "âš ï¸ Admin user creation skipped (may already exist)"

            # Drop unused 'wallets' collection (one-time cleanup)
            echo "ğŸ§¹ Dropping unused 'wallets' collection..."
            docker exec opd-mongodb-prod mongosh opd_wallet --quiet --eval "
              var result = db.wallets.drop();
              if (result) {
                print('âœ… Dropped unused wallets collection');
              } else {
                print('â„¹ï¸  wallets collection does not exist or already dropped');
              }
            " 2>&1 || echo "âš ï¸ Collection cleanup completed"

            # Verify Redis is running and accessible
            echo "ğŸ” Verifying Redis connection..."
            docker exec opd-redis-prod redis-cli --raw incr ping 2>&1 | head -n 1 || echo "âš ï¸ Redis health check completed"
            echo "âœ… Redis is running and accessible"

            echo "âœ… Deployment completed successfully"